library("caret")
library("boot")     # For calcualting errors from cross-validation
library("rpart")    # For random forest
library("rattle")   # For visualizing the random forest tree
library("ggplot2")  # For visualizations
library("dplyr")    # For streamlining the code for manipulating the datasset
library("cluster")
library("Rtsne")    # For visualizing the clustering in 2-D
library("tibble")
library("xgboost")  # For Xgboost
library("glmnet")   # For Lasso
library("neuralnet")# For neuralnet
library("reshape2") # For boxplots
library("readxl")
Customer_Data <- read_excel("./Customer Data.xlsx")
View(Customer_Data)
names<-c(2:8)
Customer_Data[,names]<-lapply(Customer_Data[,names],factor)
Customer_Data$INCOME_DESC<-factor(Customer_Data$INCOME_DESC, levels = c("Under 15K","15-24K","25-34K",
"35-49K","50-74K","75-99K","100-124K","125-149K","150-174K","175-199K","200-249K","250K+"))
Customer_Data_Model<-Customer_Data[,-c(1,10,11)]   # Removing FAMILY_TOT_VISITS & FAMILY_VALUE
############################## Linear regression ############################################################
lm.err<-c(NA)
for(i in 1:10){
set.seed(i)
fit<-glm(FAMILY_TOT_SALES ~ .,data = Customer_Data_Model) # Fits a linear regression
lm.err[i]<-cv.glm(Customer_Data_Model,fit,K=10)$delta[2]  # Taking the Bias adjusted error
}
lm.err<-sqrt(mean(lm.err)) # Mean Squared Error: After running 10-K fold cross-validation 10 times, very high
# Error: 1797.141
par(mfrow=c(2,2))
plot(fit)
set.seed(05052018)
index<-sample(1:nrow(Customer_Data_Model),nrow(Customer_Data_Model)*0.8,replace = TRUE)
training<-Customer_Data_Model[index,]
testing<-Customer_Data_Model[-index,]
null<-lm(FAMILY_TOT_SALES~1,data = training)
full<-lm(FAMILY_TOT_SALES~.,data = training)
step.fit<-step(null,scope = list(upper = full, lower = null),direction = "both",trace = TRUE, k = 2)
lm.AIC<-lm(FAMILY_TOT_SALES ~ INCOME_DESC + KID_CATEGORY_DESC,data=training)
lm.BIC<-lm(FAMILY_TOT_SALES ~ INCOME_DESC,data=training)
lm.AIC.err<-sqrt(sum((predict(lm.AIC,testing) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
lm.BIC.err<-sqrt(sum((predict(lm.BIC,testing) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
################################ Lasso Regression ###########################################################
y.train<-as.matrix(training[,8])   # Response in training has to be in matrix form for glmnet
x.train<-as.matrix(model.matrix(FAMILY_TOT_SALES ~.,training)[,-1])
# Predictors in training has to be in matrix form for glmnet
# model.matrix creates dummy variable for each level of each categorical variable
cv.lasso<-cv.glmnet(x = x.train,y = y.train, alpha = 1)
lasso<-glmnet(x = x.train,y = y.train, alpha = 1)
plot(cv.lasso)  # Plots: MSE vs Lambda
plot(lasso)   # Plots: Coefficients vs Lambda
best_lam<-cv.lasso$lambda.min  # Best lambda that gives minimum mse
y.test<-as.matrix(testing[,8])
coef(lasso,s=best_lam)
newdata<-as.matrix(data.frame(cbind(x.test,y.test)))
x.test<-as.matrix(model.matrix(FAMILY_TOT_SALES ~.,testing)[,-1])
x.test<-as.matrix(model.matrix(FAMILY_TOT_SALES ~.,testing)[,-1])
y.test<-as.matrix(testing[,8])
newdata<-as.matrix(data.frame(cbind(x.test,y.test)))
coef(lasso,s=best_lam)
# Marital status was removed during lasso regression
lasso.fit<-lm(FAMILY_TOT_SALES ~ . - MARITAL_STATUS_CODE, data = training)
lasso.err<-sqrt(sum((predict(lasso.fit, testing) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
rf.fit1<-rpart(FAMILY_TOT_SALES ~ .,data = training)
plotcp(rf.fit1)
# two splits are suggested based on complexity parameter
rf.fit2<-prune(rf.fit1,cp = 0.041)
fancyRpartPlot(rf.fit2)
rf.err<-sqrt(sum((predict(rf.fit2,testing) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
rf.fit3<-prune(rf.fit1,cp = 0.013)
fancyRpartPlot(rf.fit3)
rf.err<-sqrt(sum((predict(rf.fit3,testing) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
varImp(rf.fit3)
barplot(table(training$INCOME_DESC), main = "Counts of customers by Income group")
income<-Customer_Data_Model %>%
group_by(INCOME_DESC) %>%
summarize(Median_Sales_By_Income = median(FAMILY_TOT_SALES))
attach(income)
plot(INCOME_DESC,Median_Sales_By_Income, main = "Annual household sales by Income group")
abline(h=2330, col = "blue")
abline(h=3898,col = "red")
rf.err<-sqrt(sum((predict(rf.fit3,testing) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
############################### XGBoost ###################################################################
sparse_matrix <- sparse.model.matrix(FAMILY_TOT_SALES ~ .-1, data = training) # One Hot Encoding
y = training$FAMILY_TOT_SALES
xgb.fit<-xgboost(data = sparse_matrix,     # Predictors in sparse matrix
label = y,                # Response as it is
booster = "gbtree",
eta = 0.1,
max_depth = 15,           # Dept hof the tree
nround=25,                # Number of trees/iterations
nfold = 10,               # Number of folds in K-fold
objective = "reg:linear", # Default option for xgboost
eval_metric = "rmse",
nthread = 3,              # Number of cores to be run parallelly for xgboost
early.stop.round = 10     # Stop if model doesnt improve after 10 iterations
)
train.xgb<- as.matrix(training, rownames.force=NA)
test.xgb<- as.matrix(testing, rownames.force=NA)
train.xgb <- as(train.xgb, "sparseMatrix")
test.xgb <- as(test.xgb, "sparseMatrix")
# Never forget to exclude objective variable in 'data option'
train_Data <- xgb.DMatrix(data = train.xgb[,1:7], label = train.xgb[,"FAMILY_TOT_SALES"])
param<-list(
objective = "reg:linear",   # Objective to minimize
eval_metric = "rmse",       # Evaluation metric fro validating the model
booster = "gbtree",         # Default option for xgboost
max_depth = 8,              # Depth of the tree
eta = 0.123                 # Learning rate: weight
)
xgb.fit<-xgb.train(params = param,
data = train_Data,
nrounds = 200,
watchlist = list(train = train_Data),
verbose = TRUE,
print_every_n = 10,
nthread = 6)
test_data <- xgb.DMatrix(data = test.xgb[,1:7])
xgb.err<-sqrt(sum((predict(xgb.fit,test_data) - testing$FAMILY_TOT_SALES)^2)/nrow(testing))
xgb.err
m<-model.matrix( ~ AGE_DESC + MARITAL_STATUS_CODE + INCOME_DESC +
HOMEOWNER_DESC + HH_COMP_DESC + HOUSEHOLD_SIZE_DESC + KID_CATEGORY_DESC + FAMILY_TOT_SALES, data = training)[,-1]
n<-colnames(m)
# Selected significant Features from linear model
f<-as.formula("FAMILY_TOT_SALES ~ `INCOME_DESC15-24K` + `INCOME_DESC25-34K` + `INCOME_DESC35-49K`+ `INCOME_DESC50-74K` + `INCOME_DESC75-99K` + `INCOME_DESC100-124K` + `INCOME_DESC125-149K` + `INCOME_DESC150-174K` + `INCOME_DESC175-199K` + `INCOME_DESC200-249K` + `INCOME_DESC250K+` + `HOUSEHOLD_SIZE_DESC2` + `HOUSEHOLD_SIZE_DESC3` + `HOUSEHOLD_SIZE_DESC4` + `HOUSEHOLD_SIZE_DESC5+`+ `KID_CATEGORY_DESC2` + `KID_CATEGORY_DESC3+` + `KID_CATEGORY_DESCNone/Unknown`")
nn<-neuralnet(f,data=m,hidden=c(4),linear.output=T, stepmax=1e6)
# Checking with another package
nn2<-nnet(mm,m[,35],size=10,linout=T)
library("nnet")
# Checking with another package
nn2<-nnet(mm,m[,35],size=10,linout=T)
# Checking with another package
nn2<-nnet(m,m[,35],size=10,linout=T)
predict(nn2,m[,-8])
View(Customer_Data)
list.files()
##############################
## Segmentation
##############################
library("rattle")   # For visualizing the random forest tree
library("ggplot2")  # For visualizations
library("cluster")
library("Rtsne")    # For visualizing the clustering in 2-D
############################## Clustering of the Customers ################################################
## Calculate Gower Distance
gower_dist <- daisy(Customer_Data[,-1],metric = "gower", type = list(logratio = c(8,9,10)))
## Calculate optimal number of clusters
sil_width <- c(NA)
for(i in 2:20){
pam_fit<-pam(gower_dist, diss = TRUE,k = i)  # PAM: Partitioning Around Medoids
sil_width[i]<-pam_fit$silinfo$avg.width
}
tab<-data.frame(x=1:20,sil_width=sil_width)
ggplot(data=tab,aes(x = x,y = sil_width))+geom_point(cex=3,col="red")+geom_line()+ggtitle("Silhoutte Width Vs Number of clusters")+theme(plot.title = element_text(hjust=0.5))+xlab("Number of clusters")
## Creating clusters
pam_fit<-pam(gower_dist, diss=TRUE, k = 8)
Customer_Data<-cbind(Customer_Data, Group = pam_fit$clustering)
## Visualizing the clusters
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
data.frame() %>%
setNames(c("X", "Y")) %>%
mutate(cluster = factor(pam_fit$clustering),
name = Customer_Data$H_KEY)
ggplot(aes(x = X, y = Y), data = tsne_data) + geom_point(aes(color = cluster)) + ggtitle("Customer Segments") + theme(plot.title = element_text(hjust = 0.5))
Customer_Data2<-melt(Customer_Data,Group=Group,measure.vars = FAMILY_TOT_SALES)
install.packages("odbc")
install.packages("RODBC")
library("RODBC")
dbconnection <- odbcDriverConnect("Driver=ODBC Driver 11 for SQL Server;Server=SCOTT\\SQLEXPRESS; Database=SUPERMARKET;Uid=; Pwd=; trusted_connection=yes")
initdata <- sqlQuery(dbconnection,paste("select * from RFM;"))
View(initdata)
dbconnection <- odbcDriverConnect("Driver=ODBC Driver 11 for SQL Server;Server=SCOTT\\SQLEXPRESS; Database=SUPERMARKET;Uid=; Pwd=; trusted_connection=yes")
Customer_Data <- sqlQuery(dbconnection,paste("select * from RFM;"))
View(Customer_Data)
channel
odbcClose(channel)
dbconnection
odbcClose(dbconnection)
View(Customer_Data)
############################## Clustering of the Customers ################################################
## Calculate Gower Distance
gower_dist <- daisy(Customer_Data[,-1],metric = "gower", type = list(logratio = c(8:13)))
## Calculate optimal number of clusters
sil_width <- c(NA)
for(i in 2:20){
pam_fit<-pam(gower_dist, diss = TRUE,k = i)  # PAM: Partitioning Around Medoids
sil_width[i]<-pam_fit$silinfo$avg.width
}
tab<-data.frame(x=1:20,sil_width=sil_width)
ggplot(data=tab,aes(x = x,y = sil_width))+geom_point(cex=3,col="red")+geom_line()+ggtitle("Silhoutte Width Vs Number of clusters")+theme(plot.title = element_text(hjust=0.5))+xlab("Number of clusters")
## Creating clusters
pam_fit<-pam(gower_dist, diss=TRUE, k = 7)
Customer_Data<-cbind(Customer_Data, Group = pam_fit$clustering)
## Visualizing the clusters
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
data.frame() %>%
setNames(c("X", "Y")) %>%
mutate(cluster = factor(pam_fit$clustering),
name = Customer_Data$H_KEY)
ggplot(aes(x = X, y = Y), data = tsne_data) + geom_point(aes(color = cluster)) + ggtitle("Customer Segments") + theme(plot.title = element_text(hjust = 0.5))
Customer_Data2<-melt(Customer_Data,Group=Group,measure.vars = FAMILY_TOT_SALES)
Customer_Data2<-melt(Customer_Data,Group=Group,measure.vars = ANNUAL_SALES)
names(Customer_Data)
Customer_Data2<-melt(Customer_Data,Group=Group,measure.vars = ANNUAL_SALES)
melt(Customer_Data,Group=Group,measure.vars = ANNUAL_SALES)
Customer_Data2<-melt(Customer_Data,Group=Group,measure.vars = Customer_Data$ANNUAL_SALES)
